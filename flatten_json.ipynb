{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c4f6a37-2e9c-4860-b8ad-bac8ea1d3343",
     "showTitle": true,
     "title": "Libraries"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from array import *\n",
    "import time\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2b8bcb7-c5f7-48f2-9f63-973a006fe872",
     "showTitle": true,
     "title": "Rename Df Columns"
    }
   },
   "outputs": [],
   "source": [
    "def rename_cols(df: DataFrame, col_names: Dict[str,str]) ->DataFrame:\n",
    "    return df.select(*[col(col_name).alias(col_names.get(col_name, col_name)) for col_name in df.columns])\n",
    "\n",
    "def update_column_names(df:DataFrame, index: int) -> DataFrame:\n",
    "    df_temp = df\n",
    "    all_cols = df_temp.columns\n",
    "    new_cols = dict((column, f\"{column}*{index}\") for column in all_cols)\n",
    "    df_tmp = df_temp.transform(lambda df_x: rename_cols(df_x, new_cols))\n",
    "    #print(all_cols)\n",
    "    #print(new_cols)\n",
    "    #display(df_temp)\n",
    "    return df_temp\n",
    "\n",
    "def final_column_names(df:DataFrame, stringToDelete) -> DataFrame:\n",
    "    df_temp = df\n",
    "    all_cols = df_temp.columns\n",
    "    new_cols = dict((column, column.replace(stringToDelete, '').lower()) for column in all_cols)\n",
    "    df_tmp = df_temp.transform(lambda df_x: rename_cols(df_x, new_cols))\n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2b4a54b-fead-4e32-a28b-07bdcdf84d13",
     "showTitle": true,
     "title": "Flatten JSON"
    }
   },
   "outputs": [],
   "source": [
    "#The function will flaten the JSON, but it CANNOT be applied to the full json because there are atributes with the same name\n",
    "def flatten_json(df_arg: DataFrame, index: int = 1) -> DataFrame:\n",
    "    \n",
    "    #display(df_arg)\n",
    "    df = update_column_names(df_arg, index) if index==1 else df_arg\n",
    "    #display(df)\n",
    "\n",
    "    #get structure of json\n",
    "    fields = df.schema.fields\n",
    "    #print(fields)\n",
    "\n",
    "    #For each atribute in the JSON\n",
    "    for field in fields:\n",
    "        #get the data type and column name\n",
    "        data_type = str(field.dataType)\n",
    "        column_name = field.name\n",
    "\n",
    "        #The first 10 chars will show if the current section has nested atributes (if it is ArrayType)\n",
    "        first_10_chars = data_type[0:10]\n",
    "\n",
    "        #If it has nested atributies it will explode another level and run the function again\n",
    "        if first_10_chars == 'ArrayType(':\n",
    "            df_temp = df.withColumn(column_name, explode_outer(col(column_name)))\n",
    "            return flatten_json(df_temp, index + 1)\n",
    "\n",
    "        #otherwise \n",
    "        elif first_10_chars == 'StructType':\n",
    "            current_col = column_name\n",
    "            append_str = current_col\n",
    "\n",
    "            #all data types of the atributes of that level:\n",
    "            data_type_str = str(df.schema[current_col].dataType)\n",
    "\n",
    "            df_temp = df.withColumnRenamed(column_name, column_name + \"#1\") \\\n",
    "                if column_name in data_type_str else df\n",
    "            current_col = current_col + \"#1\" if column_name in data_type_str else current_col\n",
    "\n",
    "            df_before_expanding = df_temp.select(f\"{current_col}.*\")\n",
    "            newly_gen_cols = df_before_expanding.columns\n",
    "            \n",
    "            begin_index = append_str.rfind('*')\n",
    "            end_index = len(append_str)\n",
    "\n",
    "            custom_cols = dict((field, f\"{append_str}_{field}\") for field in newly_gen_cols)\n",
    "            df_temp2 = df_temp.select(\"*\", f\"{current_col}.*\").drop(current_col)\n",
    "            df_temp3 = df_temp2.transform(lambda df_x: rename_cols(df_x, custom_cols))\n",
    "            return flatten_json(df_temp3, index + 1)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01.CommonHeader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
